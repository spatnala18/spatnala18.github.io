<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sanjay Kumar Patnala - Resume</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>SANJAY KUMAR PATNALA</h1>
            <div class="contact">
                <p>33 11th St NE, Atlanta, GA 30309 | +1470.408.8611</p>
                <p>
                    <a href="mailto:sanjaypatnala3@gmail.com">sanjaypatnala3@gmail.com</a> |
                    <a href="https://www.linkedin.com/in/YOUR-LINKEDIN-URL">LinkedIn</a> |
                    <a href="https://github.com/spatnala18">Github</a>
                </p>
                <p class="pdf-link"><a href="resume_sanjaypatnala_llm_systems.pdf">[ Download PDF Version ]</a></p>
            </div>
        </header>

        <section>
            <h2>EDUCATION</h2>
            <div class="entry">
                <h3>GEORGIA INSTITUTE OF TECHNOLOGY (Atlanta, USA)</h3>
                <p class="date">Aug 2024 - May 2026 (Expected)</p>
                <p><strong>Master of Science in Computer Science (ML Specialization)</strong>; GPA: 3.9/4.0</p>
                <p><em>Coursework:</em> Machine Learning, Graph ML, Deep RL, ML Security, HW SW co-design- ML systems, GPU HW&SW, Systems ML</p>
            </div>
            <div class="entry">
                <h3>Indian Institute of Technology Bombay (Mumbai, India)</h3>
                <p class="date">July 2017 - Aug 2021</p>
                <p><strong>Bachelor of Technology in Mechanical Engineering</strong>; GPA: 8.32/10.0</p>
                <p><em>Coursework:</em> Data Mining, Deep Learning, Advanced Deep Learning, Deep Learning for NLP, Linear Algebra</p>
            </div>
        </section>

        <section>
            <h2>PROJECTS</h2>
            <div class="entry">
                <h3>KV-Drip for Tail-Latency Reduction in LLM Inference</h3>
                <p class="date">Aug 2025-current</p>
                <ul>
                    <li>Implemented token-level hedged decoding on disaggregated P/D inference engine on vLLM, streaming partial KV-cache states between GPUs to rescue slow decodes in real time; reduced p95 latency by 47% on 4×H100 long-context runs.</li>
                </ul>
            </div>
            <div class="entry">
                <h3>LLM Fine-Tuning and Distributed Inference Optimization | HW SW Co-Design for ML sys</h3>
                <p class="date">Jan 2025-Apr 2025</p>
                <ul>
                    <li>Fine-tuned LLaMA-7B across 4×H100 (NVLink) using PyTorch DDP (NCCL) and a Triton Flash-Attn (QKT-softmax-V) SDPA backend; tuned global batch/grad-accumulation via a simple roofline model resulting in +26% training throughput</li>
                    <li>Engineered a block-paged, sharded KV pipeline with t+1 prefetch on dual copy engines and NCCL overlap to localize KV reads and minimize cross-GPU hops; reduced p95 H2D memcpy time by 35%</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>WORK EXPERIENCE</h2>
            <div class="entry">
                <h3>Amazon (Alexa AGI) - Machine Learning Intern (Seattle, WA)</h3>
                <p class="date">May 2025-Aug 2025</p>
                <ul>
                    <li>Implemented a conversational feature recommender RAG stack on AWS g6 (L4) with BGE-large embeddings and an LLaMA-8B selector on the same node, cutting retrieval latency by 25% vs Bedrock API calls.</li>
                    <li>Optimized Llama-3.3-70B on AWS p5 (H100) using TensorRT-LLM FP8, speculative decoding, paged and quantized KV-cache, and in-flight batching, delivering around 2.2x throughput; TTFT of 800ms p95, TBT of 65ms p95 (16 tok/s)</li>
                    <li>Designed an attention-entropy aware KV policy to prefetch hot KV and quantize and defer cold blocks (nf4 on host, int8 on device), cutting KV transfer volume by 28% with no acc loss on QA/RAG evals</li>
                </ul>
            </div>
            <div class="entry">
                <h3>Mastercard Al Garage - Data Scientist (Gurugram, India)</h3>
                <p class="date">Feb 2023-July 2024</p>
                <ul>
                    <li>Developed an NL2SQL system using RAG with DPR and FAISS vector search; Fine-tuned LLAMA 7B with QLORA-based PEFT to generate precise SQL queries, achieving 87% query accuracy by retrieving relevant database schemas and past queries</li>
                    <li>Built an in-house hyperparameter tuning system that focuses on diversity and label imbalance, saving an estimated $3 million by reducing dependency on costly external packages such as H2O</li>
                </ul>
            </div>
            <div class="entry">
                <h3>Ubisoft India Studios - Programmer (Mumbai, India)</h3>
                <p class="date">July 2021-Sept 2022</p>
                <ul>
                    <li>Parallelized the engine's Al loop in C++ (job system, lock-free queues), reducing main-thread stalls and frame-time variance</li>
                    <li>Profiled and tuned CUDA kernels with Nsight Compute, reducing warp divergence, occupancy limits and memory stalls</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>PUBLICATIONS</h2>
            <div class="entry">
                <ul>
                    <li>Sanjay Patnala. "CALO-GNN: Calibrated-Uncertainty Graph Cost Models for Cross-Device TVM Meta-Schedule." KDD 2025 Workshop on Inference Optimization for GenAl</li>
                    <li>Sanjay Patnala, Ushmita Pareek et. al. "GT2MLP: Attention aided distillation from Graph Transformer to MLP for Graph Structure-Independent Inference." CODS COMAD 2025 (Best Reviewed Paper)</li>
                    <li>Pranav Poduval, Sanjay Patnala et. al. "CASH via Optimal Diversity for Ensemble Learning." KDD 2024</li>
                    <li>Sumedh B.G, Sanjay Patnala et.al. "Local and Global structure aware Graph Transformer for Node classification". NeurIPS 2023 Workshop on Graph Learning Frontiers</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>SKILLS</h2>
            <div class="entry">
                <ul class="skills">
                    <li><strong>Programming:</strong> Python, C++, CUDA, Triton, SQL, Pandas, NumPy, R, JavaScript, Apache Spark, D3, SQLite</li>
                    <li><strong>ML Tools:</strong> PyTorch, Tensorflow, ONNX, JAX, vLLM, SGLang, TensorRT, Scikit-Learn, Tableau, MapReduce, Kubernetes, Docker</li>
                    <li><strong>Software/Tools:</strong> MATLAB, Octave, AWS BedRock, AWS SageMaker, S3, DataBricks, Git, GCP, Excel</li>
                    <li><strong>Skilled Areas:</strong> Inference Optimization, ML systems, NLP, LLMS, RAG, PEFT, CASH, Statistics, A/B Testing, Forecasting, Graph ML</li>
                </ul>
            </div>
        </section>
    </div>
</body>
</html>
